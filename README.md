# Abstractive_text_summarization
A seq2seq neural network model based on multi-head self-attention mechanism at encoding stage and adopting pointer generator, coverage mechanism at decoding stage to handle out of vocabulary and repetition words.
